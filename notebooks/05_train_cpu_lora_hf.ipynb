{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e26b47",
   "metadata": {},
   "source": [
    "### **Setup & paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e048e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT    : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\n",
      "SFT_DIR : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\data\\sft\n",
      "OUT     : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17\n",
      "MODEL_ID: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\models\\Qwen2.5-1.5B-Instruct\n",
      "OFFLINE : 1\n",
      "Python  : 3.11.13 | Windows-10-10.0.26100-SP0\n"
     ]
    }
   ],
   "source": [
    "# ===== A) Paths & offline mode =====\n",
    "from pathlib import Path\n",
    "import sys, os, platform\n",
    "\n",
    "CWD  = Path.cwd().resolve()\n",
    "ROOT = CWD if (CWD / \"src\").exists() else CWD.parent\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "DATA    = ROOT / \"data\"\n",
    "SFT_DIR = DATA / \"sft\"\n",
    "OUT     = ROOT / \"outputs\" / \"lora_hf\" / \"title17\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCAL_QWEN = ROOT / \"models\" / \"Qwen2.5-1.5B-Instruct\"\n",
    "LOCAL_QWEN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# This is the ONLY place we set these:\n",
    "MODEL_ID = str(LOCAL_QWEN)        # <— local path, not hub id\n",
    "LOCAL_FILES_ONLY = True\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"  # extra-safe offline\n",
    "\n",
    "print(\"ROOT    :\", ROOT)\n",
    "print(\"SFT_DIR :\", SFT_DIR)\n",
    "print(\"OUT     :\", OUT)\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n",
    "print(\"OFFLINE :\", os.environ.get(\"TRANSFORMERS_OFFLINE\"))\n",
    "print(\"Python  :\", sys.version.split()[0], \"|\", platform.platform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea3f8e",
   "metadata": {},
   "source": [
    "### **Model Download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995d83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c7c915dceb41afb8489a8dfefe04d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pdf-agent-2\\Lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9128049ed3aa4bcb8411b8fe6d5e5875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf8e1cc7a484fbab938c22e6965f6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56976ec41c2d40129c86274a86075f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7594b1d4d54148f38313b6f66be56e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bedb38f737f4a95a2ea5609aa777f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fcd81303304f29b46b771abc8d13c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332443336d4244acbecde16f7e6792f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model dir: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\models\\Qwen2.5-1.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# from pathlib import Path\n",
    "\n",
    "# LOCAL_QWEN = Path(ROOT / \"models\" / \"Qwen2.5-1.5B-Instruct\")\n",
    "# LOCAL_QWEN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# local_dir = snapshot_download(\n",
    "#     repo_id=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "#     local_dir=str(LOCAL_QWEN),\n",
    "#     local_dir_use_symlinks=False,  # avoid symlink perms on Windows\n",
    "#     allow_patterns=[\"*.json\",\"*.bin\",\"*.safetensors\",\"*.model\",\"tokenizer*\",\"*.py\",\"*.txt\"],\n",
    "# )\n",
    "\n",
    "# print(\"Local model dir:\", local_dir)\n",
    "\n",
    "# # Switch to fully offline usage:\n",
    "# MODEL_ID = str(LOCAL_QWEN)\n",
    "# LOCAL_FILES_ONLY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdca691",
   "metadata": {},
   "source": [
    "### **Check/prepare Alpaca data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66aa2e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPACA exists: True | D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\data\\sft\\alpaca.train.jsonl\n",
      "{'instruction': '(factual) What is the main rule in Copyright Law United States Copyri > § 115 · Scope of exclusive rights in nondramatic musical works: Compulsory license for making and distributing phonorecords 50?\\nPrefer concise, correct answers. End with [pp. 88–92].', 'input': '', 'output': 'The primary purpose must be to distribute them to the public for private use.'}\n",
      "{'instruction': 'Summarize the section:\\nHeading: Copyright Law United States Copyri > § 304 · Duration of copyright: Subsisting copyrights 6\\nSummarize as 6–8 concise bullet points. End with [pp. 189–201].', 'input': '', 'output': '- - For copyrights subsisting on January 1, 1978, the original term endures for 28 years.\\n- - In certain cases, such as posthumous works or corporate copyrights, a renewal and extension of 67 years is possible.\\n- - Eligible claimants include the author if living, their widow/widower or children if deceased, or executors/next of kin in the absence of a will.'}\n"
     ]
    }
   ],
   "source": [
    "# ===== B) Ensure alpaca jsonl exists & peek =====\n",
    "from src.train.sft_to_alpaca import to_alpaca\n",
    "ALPACA = SFT_DIR / \"alpaca.train.jsonl\"\n",
    "\n",
    "if not ALPACA.exists():\n",
    "    src_train = SFT_DIR / \"train.jsonl\"\n",
    "    assert src_train.exists(), f\"Missing SFT source: {src_train}\"\n",
    "    to_alpaca(src_train, ALPACA)\n",
    "\n",
    "print(\"ALPACA exists:\", ALPACA.exists(), \"|\", ALPACA)\n",
    "\n",
    "import json, itertools\n",
    "for line in itertools.islice(open(ALPACA, \"r\", encoding=\"utf-8\"), 2):\n",
    "    print(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229ff53",
   "metadata": {},
   "source": [
    "### **Tokenizer smoke test (catches HF auth/issues early)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d10f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer OK. eos: <|im_end|> | pad: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# ===== D) Verify tokenizer loads strictly offline =====\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, local_files_only=LOCAL_FILES_ONLY, trust_remote_code=True\n",
    ")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "print(\"Tokenizer OK. eos:\", tok.eos_token, \"| pad:\", tok.pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24311543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch        : 2.8.0+cpu | cuda? False\n",
      "transformers : 4.55.2\n",
      "datasets     : 4.0.0\n",
      "peft         : 0.17.1\n",
      "accelerate   : 1.10.0\n",
      "CPU threads  : 8\n"
     ]
    }
   ],
   "source": [
    "# ===== E) Lib versions =====\n",
    "import torch, transformers, datasets, peft, accelerate, os\n",
    "print(\"torch        :\", torch.__version__, \"| cuda?\", torch.cuda.is_available())\n",
    "print(\"transformers :\", transformers.__version__)\n",
    "print(\"datasets     :\", datasets.__version__)\n",
    "print(\"peft         :\", peft.__version__)\n",
    "print(\"accelerate   :\", accelerate.__version__)\n",
    "print(\"CPU threads  :\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb6407",
   "metadata": {},
   "source": [
    "### **Hyperparams (CPU-friendly)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ebbc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_steps': 300, 'batch': 1, 'grad_accum': 4, 'lr': 0.0002, 'max_seq_len': 512, 'num_threads': 8, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# ===== F) CPU-safe hparams =====\n",
    "MAX_STEPS    = 300\n",
    "BATCH_SIZE   = 1\n",
    "GRAD_ACCUM   = 4\n",
    "LR           = 2e-4\n",
    "MAX_SEQ_LEN  = 512\n",
    "NUM_THREADS  = min(8, os.cpu_count() or 8)\n",
    "LORA_R       = 8\n",
    "LORA_ALPHA   = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "SEED         = 7\n",
    "\n",
    "print(dict(max_steps=MAX_STEPS, batch=BATCH_SIZE, grad_accum=GRAD_ACCUM, lr=LR,\n",
    "           max_seq_len=MAX_SEQ_LEN, num_threads=NUM_THREADS,\n",
    "           lora_r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4982dea",
   "metadata": {},
   "source": [
    "### **Train LoRA (run script, stream logs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0739e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching: 'd:\\Anaconda\\envs\\pdf-agent-2\\python.exe' 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\src\\train\\cpu_lora_hf.py' --model_id 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\models\\Qwen2.5-1.5B-Instruct' --train_jsonl 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\data\\sft\\alpaca.train.jsonl' --out_dir 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17' --max_steps 300 --batch_size 1 --grad_accum 4 --lr 0.0002 --max_seq_len 512 --num_threads 8 --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --seed 7 --local_files_only --trust_remote_code\n",
      "\n",
      "Map:   0%|          | 0/130 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 130/130 [00:00<00:00, 153.92 examples/s]\n",
      "Map: 100%|██████████| 130/130 [00:00<00:00, 146.20 examples/s]\n",
      "trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n",
      "D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\src\\train\\cpu_lora_hf.py:152: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]d:\\Anaconda\\envs\\pdf-agent-2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\n",
      "  0%|          | 1/300 [00:52<4:20:02, 52.18s/it]\n",
      "  1%|          | 2/300 [02:24<6:15:05, 75.52s/it]\n",
      "  1%|          | 3/300 [03:20<5:31:27, 66.96s/it]\n",
      "  1%|▏         | 4/300 [04:27<5:29:51, 66.86s/it]\n",
      "  2%|▏         | 5/300 [05:28<5:18:40, 64.81s/it]\n",
      "  2%|▏         | 6/300 [06:30<5:12:35, 63.79s/it]\n",
      "  2%|▏         | 7/300 [07:41<5:23:28, 66.24s/it]\n",
      "  3%|▎         | 8/300 [08:30<4:55:13, 60.66s/it]\n",
      "  3%|▎         | 9/300 [09:26<4:46:27, 59.07s/it]\n",
      "  3%|▎         | 10/300 [10:08<4:21:06, 54.02s/it]\n",
      "                                                  \n",
      "{'loss': 2.7417, 'grad_norm': 1.3889130353927612, 'learning_rate': 0.0002, 'epoch': 0.31}\n",
      "\n",
      "  3%|▎         | 10/300 [10:09<4:21:06, 54.02s/it]\n",
      "  4%|▎         | 11/300 [11:06<4:25:36, 55.14s/it]\n",
      "  4%|▍         | 12/300 [11:58<4:19:53, 54.14s/it]\n",
      "  4%|▍         | 13/300 [12:47<4:11:43, 52.63s/it]\n",
      "  5%|▍         | 14/300 [13:54<4:31:02, 56.86s/it]\n",
      "  5%|▌         | 15/300 [15:03<4:48:13, 60.68s/it]\n",
      "  5%|▌         | 16/300 [16:20<5:10:00, 65.50s/it]\n",
      "  6%|▌         | 17/300 [17:29<5:14:33, 66.69s/it]\n",
      "  6%|▌         | 18/300 [18:13<4:41:22, 59.87s/it]\n",
      "  6%|▋         | 19/300 [19:15<4:42:35, 60.34s/it]\n",
      "  7%|▋         | 20/300 [19:55<4:13:18, 54.28s/it]\n",
      "                                                  \n",
      "{'loss': 1.8492, 'grad_norm': 2.2627577781677246, 'learning_rate': 0.00019312714776632305, 'epoch': 0.62}\n",
      "\n",
      "  7%|▋         | 20/300 [19:55<4:13:18, 54.28s/it]\n",
      "  7%|▋         | 21/300 [20:49<4:12:12, 54.24s/it]\n",
      "  7%|▋         | 22/300 [21:37<4:02:04, 52.25s/it]\n",
      "  8%|▊         | 23/300 [22:24<3:54:29, 50.79s/it]\n",
      "  8%|▊         | 24/300 [23:10<3:47:11, 49.39s/it]\n",
      "  8%|▊         | 25/300 [24:10<4:00:21, 52.44s/it]\n",
      "  9%|▊         | 26/300 [25:09<4:08:31, 54.42s/it]\n",
      "  9%|▉         | 27/300 [26:07<4:13:12, 55.65s/it]\n",
      "  9%|▉         | 28/300 [27:04<4:13:51, 56.00s/it]\n",
      " 10%|▉         | 29/300 [27:51<4:00:09, 53.17s/it]\n",
      " 10%|█         | 30/300 [28:39<3:52:09, 51.59s/it]\n",
      "                                                  \n",
      "{'loss': 1.3258, 'grad_norm': 1.2689427137374878, 'learning_rate': 0.00018625429553264605, 'epoch': 0.92}\n",
      "\n",
      " 10%|█         | 30/300 [28:39<3:52:09, 51.59s/it]\n",
      " 10%|█         | 31/300 [29:43<4:08:38, 55.46s/it]\n",
      " 11%|█         | 32/300 [30:34<4:01:21, 54.04s/it]\n",
      " 11%|█         | 33/300 [30:57<3:19:47, 44.90s/it]\n",
      " 11%|█▏        | 34/300 [31:56<3:37:12, 48.99s/it]\n",
      " 12%|█▏        | 35/300 [32:45<3:36:38, 49.05s/it]\n",
      " 12%|█▏        | 36/300 [33:30<3:30:56, 47.94s/it]\n",
      " 12%|█▏        | 37/300 [34:17<3:27:44, 47.39s/it]\n",
      " 13%|█▎        | 38/300 [35:17<3:43:52, 51.27s/it]\n",
      " 13%|█▎        | 39/300 [36:14<3:50:32, 53.00s/it]\n",
      " 13%|█▎        | 40/300 [37:13<3:57:44, 54.86s/it]\n",
      "                                                  \n",
      "{'loss': 1.1589, 'grad_norm': 1.268571138381958, 'learning_rate': 0.0001793814432989691, 'epoch': 1.22}\n",
      "\n",
      " 13%|█▎        | 40/300 [37:13<3:57:44, 54.86s/it]\n",
      " 14%|█▎        | 41/300 [38:15<4:06:35, 57.13s/it]\n",
      " 14%|█▍        | 42/300 [39:11<4:03:28, 56.62s/it]\n",
      " 14%|█▍        | 43/300 [40:12<4:07:38, 57.82s/it]\n",
      " 15%|█▍        | 44/300 [41:13<4:11:07, 58.86s/it]\n",
      " 15%|█▌        | 45/300 [42:03<3:59:31, 56.36s/it]\n",
      " 15%|█▌        | 46/300 [43:12<4:14:00, 60.00s/it]\n",
      " 16%|█▌        | 47/300 [44:13<4:14:27, 60.35s/it]\n",
      " 16%|█▌        | 48/300 [44:59<3:55:50, 56.15s/it]\n",
      " 16%|█▋        | 49/300 [45:41<3:36:57, 51.86s/it]\n",
      " 17%|█▋        | 50/300 [46:45<3:50:33, 55.34s/it]\n",
      "                                                  \n",
      "{'loss': 1.0696, 'grad_norm': 1.0532176494598389, 'learning_rate': 0.00017250859106529212, 'epoch': 1.52}\n",
      "\n",
      " 17%|█▋        | 50/300 [46:45<3:50:33, 55.34s/it]\n",
      " 17%|█▋        | 51/300 [47:31<3:38:36, 52.68s/it]\n",
      " 17%|█▋        | 52/300 [48:48<4:07:45, 59.94s/it]\n",
      " 18%|█▊        | 53/300 [49:49<4:07:35, 60.14s/it]\n",
      " 18%|█▊        | 54/300 [50:41<3:56:23, 57.66s/it]\n",
      " 18%|█▊        | 55/300 [51:29<3:43:50, 54.82s/it]\n",
      " 19%|█▊        | 56/300 [52:38<4:00:29, 59.14s/it]\n",
      " 19%|█▉        | 57/300 [53:24<3:43:40, 55.23s/it]\n",
      " 19%|█▉        | 58/300 [54:19<3:41:54, 55.02s/it]\n",
      " 20%|█▉        | 59/300 [55:23<3:52:43, 57.94s/it]\n",
      " 20%|██        | 60/300 [56:17<3:46:36, 56.65s/it]\n",
      "                                                  \n",
      "{'loss': 0.901, 'grad_norm': 1.2068760395050049, 'learning_rate': 0.00016563573883161513, 'epoch': 1.83}\n",
      "\n",
      " 20%|██        | 60/300 [56:17<3:46:36, 56.65s/it]\n",
      " 20%|██        | 61/300 [57:15<3:47:35, 57.14s/it]\n",
      " 21%|██        | 62/300 [58:13<3:46:51, 57.19s/it]\n",
      " 21%|██        | 63/300 [58:55<3:28:19, 52.74s/it]\n",
      " 21%|██▏       | 64/300 [59:53<3:34:01, 54.41s/it]\n",
      " 22%|██▏       | 65/300 [1:00:45<3:30:31, 53.75s/it]\n",
      " 22%|██▏       | 66/300 [1:01:13<2:59:32, 46.04s/it]\n",
      " 22%|██▏       | 67/300 [1:01:58<2:56:38, 45.49s/it]\n",
      " 23%|██▎       | 68/300 [1:03:03<3:19:28, 51.59s/it]\n",
      " 23%|██▎       | 69/300 [1:03:51<3:13:26, 50.25s/it]\n",
      " 23%|██▎       | 70/300 [1:04:42<3:13:24, 50.45s/it]\n",
      "                                                    \n",
      "{'loss': 0.8095, 'grad_norm': 1.4067201614379883, 'learning_rate': 0.00015876288659793814, 'epoch': 2.12}\n",
      "\n",
      " 23%|██▎       | 70/300 [1:04:42<3:13:24, 50.45s/it]\n",
      " 24%|██▎       | 71/300 [1:05:25<3:05:06, 48.50s/it]\n",
      " 24%|██▍       | 72/300 [1:06:24<3:15:40, 51.49s/it]\n",
      " 24%|██▍       | 73/300 [1:07:22<3:21:45, 53.33s/it]\n",
      " 25%|██▍       | 74/300 [1:08:29<3:36:17, 57.42s/it]\n",
      " 25%|██▌       | 75/300 [1:09:14<3:21:21, 53.69s/it]\n",
      " 25%|██▌       | 76/300 [1:10:15<3:28:52, 55.95s/it]\n",
      " 26%|██▌       | 77/300 [1:11:14<3:31:21, 56.87s/it]\n",
      " 26%|██▌       | 78/300 [1:12:13<3:32:59, 57.56s/it]\n",
      " 26%|██▋       | 79/300 [1:13:28<3:51:53, 62.96s/it]\n",
      " 27%|██▋       | 80/300 [1:14:25<3:43:12, 60.88s/it]\n",
      "                                                    \n",
      "{'loss': 0.6523, 'grad_norm': 1.6358766555786133, 'learning_rate': 0.00015189003436426118, 'epoch': 2.43}\n",
      "\n",
      " 27%|██▋       | 80/300 [1:14:25<3:43:12, 60.88s/it]\n",
      " 27%|██▋       | 81/300 [1:15:10<3:25:50, 56.39s/it]\n",
      " 27%|██▋       | 82/300 [1:16:03<3:20:48, 55.27s/it]\n",
      " 28%|██▊       | 83/300 [1:16:46<3:06:21, 51.53s/it]\n",
      " 28%|██▊       | 84/300 [1:17:34<3:01:53, 50.53s/it]\n",
      " 28%|██▊       | 85/300 [1:18:38<3:15:13, 54.48s/it]\n",
      " 29%|██▊       | 86/300 [1:19:51<3:34:07, 60.03s/it]\n",
      " 29%|██▉       | 87/300 [1:20:37<3:18:03, 55.79s/it]\n",
      " 29%|██▉       | 88/300 [1:22:00<3:46:39, 64.15s/it]\n",
      " 30%|██▉       | 89/300 [1:23:01<3:41:52, 63.09s/it]\n",
      " 30%|███       | 90/300 [1:23:54<3:30:41, 60.20s/it]\n",
      "                                                    \n",
      "{'loss': 0.5799, 'grad_norm': 1.5892581939697266, 'learning_rate': 0.00014501718213058418, 'epoch': 2.74}\n",
      "\n",
      " 30%|███       | 90/300 [1:23:54<3:30:41, 60.20s/it]\n",
      " 30%|███       | 91/300 [1:24:56<3:30:49, 60.53s/it]\n",
      " 31%|███       | 92/300 [1:26:28<4:02:40, 70.00s/it]\n",
      " 31%|███       | 93/300 [1:27:45<4:08:47, 72.12s/it]\n",
      " 31%|███▏      | 94/300 [1:28:37<3:47:11, 66.17s/it]\n",
      " 32%|███▏      | 95/300 [1:29:27<3:29:02, 61.18s/it]\n",
      " 32%|███▏      | 96/300 [1:30:24<3:23:52, 59.96s/it]\n",
      " 32%|███▏      | 97/300 [1:31:22<3:20:53, 59.38s/it]\n",
      " 33%|███▎      | 98/300 [1:32:15<3:13:25, 57.45s/it]\n",
      " 33%|███▎      | 99/300 [1:32:51<2:50:54, 51.02s/it]\n",
      " 33%|███▎      | 100/300 [1:34:12<3:19:58, 59.99s/it]\n",
      "                                                     \n",
      "{'loss': 0.5658, 'grad_norm': 1.298962950706482, 'learning_rate': 0.00013814432989690722, 'epoch': 3.03}\n",
      "\n",
      " 33%|███▎      | 100/300 [1:34:12<3:19:58, 59.99s/it]\n",
      " 34%|███▎      | 101/300 [1:35:03<3:10:44, 57.51s/it]\n",
      " 34%|███▍      | 102/300 [1:35:58<3:06:50, 56.62s/it]\n",
      " 34%|███▍      | 103/300 [1:37:00<3:11:00, 58.18s/it]\n",
      " 35%|███▍      | 104/300 [1:38:24<3:35:25, 65.95s/it]\n",
      " 35%|███▌      | 105/300 [1:39:19<3:23:53, 62.73s/it]\n",
      " 35%|███▌      | 106/300 [1:40:06<3:07:15, 57.91s/it]\n",
      " 36%|███▌      | 107/300 [1:41:11<3:13:02, 60.02s/it]\n",
      " 36%|███▌      | 108/300 [1:41:58<2:59:49, 56.20s/it]\n",
      " 36%|███▋      | 109/300 [1:43:03<3:07:06, 58.78s/it]\n",
      " 37%|███▋      | 110/300 [1:43:50<2:55:23, 55.39s/it]\n",
      "                                                     \n",
      "{'loss': 0.374, 'grad_norm': 1.8935503959655762, 'learning_rate': 0.00013127147766323026, 'epoch': 3.34}\n",
      "\n",
      " 37%|███▋      | 110/300 [1:43:50<2:55:23, 55.39s/it]\n",
      " 37%|███▋      | 111/300 [1:44:37<2:46:31, 52.87s/it]\n",
      " 37%|███▋      | 112/300 [1:45:25<2:40:46, 51.31s/it]\n",
      " 38%|███▊      | 113/300 [1:46:11<2:35:06, 49.77s/it]\n",
      " 38%|███▊      | 114/300 [1:46:59<2:32:37, 49.23s/it]\n",
      " 38%|███▊      | 115/300 [1:47:39<2:22:57, 46.36s/it]\n",
      " 39%|███▊      | 116/300 [1:48:47<2:41:53, 52.79s/it]\n",
      " 39%|███▉      | 117/300 [1:50:02<3:01:30, 59.51s/it]\n",
      " 39%|███▉      | 118/300 [1:51:00<2:59:36, 59.21s/it]\n",
      " 40%|███▉      | 119/300 [1:51:55<2:55:00, 58.01s/it]\n",
      " 40%|████      | 120/300 [1:52:51<2:52:08, 57.38s/it]\n",
      "                                                     \n",
      "{'loss': 0.341, 'grad_norm': 2.423497200012207, 'learning_rate': 0.00012439862542955326, 'epoch': 3.65}\n",
      "\n",
      " 40%|████      | 120/300 [1:52:51<2:52:08, 57.38s/it]\n",
      " 40%|████      | 121/300 [1:53:35<2:38:49, 53.24s/it]\n",
      " 41%|████      | 122/300 [1:54:39<2:47:32, 56.48s/it]\n",
      " 41%|████      | 123/300 [1:55:24<2:36:56, 53.20s/it]\n",
      " 41%|████▏     | 124/300 [1:56:06<2:26:04, 49.80s/it]\n",
      " 42%|████▏     | 125/300 [1:56:58<2:27:17, 50.50s/it]\n",
      " 42%|████▏     | 126/300 [1:58:17<2:51:01, 58.98s/it]\n",
      " 42%|████▏     | 127/300 [1:59:24<2:57:00, 61.39s/it]\n",
      " 43%|████▎     | 128/300 [2:00:26<2:56:39, 61.63s/it]\n",
      " 43%|████▎     | 129/300 [2:01:19<2:47:30, 58.78s/it]\n",
      " 43%|████▎     | 130/300 [2:02:06<2:37:05, 55.44s/it]\n",
      "                                                     \n",
      "{'loss': 0.3693, 'grad_norm': 2.2982068061828613, 'learning_rate': 0.0001175257731958763, 'epoch': 3.95}\n",
      "\n",
      " 43%|████▎     | 130/300 [2:02:06<2:37:05, 55.44s/it]\n",
      " 44%|████▎     | 131/300 [2:03:05<2:39:03, 56.47s/it]\n",
      " 44%|████▍     | 132/300 [2:03:33<2:14:27, 48.02s/it]\n",
      " 44%|████▍     | 133/300 [2:04:28<2:18:54, 49.91s/it]\n",
      " 45%|████▍     | 134/300 [2:05:25<2:23:55, 52.02s/it]\n",
      " 45%|████▌     | 135/300 [2:06:21<2:26:50, 53.40s/it]\n",
      " 45%|████▌     | 136/300 [2:07:13<2:24:55, 53.02s/it]\n",
      " 46%|████▌     | 137/300 [2:08:16<2:31:58, 55.94s/it]\n",
      " 46%|████▌     | 138/300 [2:09:12<2:30:41, 55.81s/it]\n",
      " 46%|████▋     | 139/300 [2:09:58<2:22:23, 53.06s/it]\n",
      " 47%|████▋     | 140/300 [2:10:42<2:14:08, 50.30s/it]\n",
      "                                                     \n",
      "{'loss': 0.244, 'grad_norm': 2.6394221782684326, 'learning_rate': 0.00011065292096219932, 'epoch': 4.25}\n",
      "\n",
      " 47%|████▋     | 140/300 [2:10:42<2:14:08, 50.30s/it]\n",
      " 47%|████▋     | 141/300 [2:11:47<2:24:48, 54.65s/it]\n",
      " 47%|████▋     | 142/300 [2:12:31<2:15:46, 51.56s/it]\n",
      " 48%|████▊     | 143/300 [2:13:23<2:14:45, 51.50s/it]\n",
      " 48%|████▊     | 144/300 [2:14:19<2:17:22, 52.84s/it]\n",
      " 48%|████▊     | 145/300 [2:15:19<2:22:17, 55.08s/it]\n",
      " 49%|████▊     | 146/300 [2:16:06<2:15:21, 52.74s/it]\n",
      " 49%|████▉     | 147/300 [2:16:58<2:14:00, 52.56s/it]\n",
      " 49%|████▉     | 148/300 [2:17:52<2:13:43, 52.79s/it]\n",
      " 50%|████▉     | 149/300 [2:18:35<2:05:19, 49.80s/it]\n",
      " 50%|█████     | 150/300 [2:19:43<2:18:32, 55.42s/it]\n",
      "                                                     \n",
      "{'loss': 0.205, 'grad_norm': 1.3130940198898315, 'learning_rate': 0.00010378006872852236, 'epoch': 4.55}\n",
      "\n",
      " 50%|█████     | 150/300 [2:19:43<2:18:32, 55.42s/it]\n",
      " 50%|█████     | 151/300 [2:20:32<2:12:52, 53.50s/it]\n",
      " 51%|█████     | 152/300 [2:21:27<2:13:08, 53.98s/it]\n",
      " 51%|█████     | 153/300 [2:22:15<2:07:50, 52.18s/it]\n",
      " 51%|█████▏    | 154/300 [2:23:23<2:18:24, 56.88s/it]\n",
      " 52%|█████▏    | 155/300 [2:24:26<2:21:38, 58.61s/it]\n",
      " 52%|█████▏    | 156/300 [2:25:24<2:20:08, 58.39s/it]\n",
      " 52%|█████▏    | 157/300 [2:26:18<2:16:40, 57.35s/it]\n",
      " 53%|█████▎    | 158/300 [2:27:14<2:14:18, 56.75s/it]\n",
      " 53%|█████▎    | 159/300 [2:27:57<2:03:26, 52.53s/it]\n",
      " 53%|█████▎    | 160/300 [2:29:09<2:16:25, 58.47s/it]\n",
      "                                                     \n",
      "{'loss': 0.2183, 'grad_norm': 1.726936936378479, 'learning_rate': 9.690721649484537e-05, 'epoch': 4.86}\n",
      "\n",
      " 53%|█████▎    | 160/300 [2:29:09<2:16:25, 58.47s/it]\n",
      " 54%|█████▎    | 161/300 [2:30:19<2:23:21, 61.88s/it]\n",
      " 54%|█████▍    | 162/300 [2:31:33<2:31:00, 65.66s/it]\n",
      " 54%|█████▍    | 163/300 [2:32:22<2:18:15, 60.55s/it]\n",
      " 55%|█████▍    | 164/300 [2:33:40<2:29:24, 65.92s/it]\n",
      " 55%|█████▌    | 165/300 [2:34:06<2:01:17, 53.91s/it]\n",
      " 55%|█████▌    | 166/300 [2:35:09<2:06:27, 56.63s/it]\n",
      " 56%|█████▌    | 167/300 [2:36:14<2:10:50, 59.03s/it]\n",
      " 56%|█████▌    | 168/300 [2:37:33<2:23:09, 65.07s/it]\n",
      " 56%|█████▋    | 169/300 [2:38:38<2:21:50, 64.97s/it]\n",
      " 57%|█████▋    | 170/300 [2:39:31<2:13:32, 61.63s/it]\n",
      "                                                     \n",
      "{'loss': 0.1837, 'grad_norm': 1.4387630224227905, 'learning_rate': 9.003436426116839e-05, 'epoch': 5.15}\n",
      "\n",
      " 57%|█████▋    | 170/300 [2:39:31<2:13:32, 61.63s/it]\n",
      " 57%|█████▋    | 171/300 [2:40:24<2:06:34, 58.87s/it]\n",
      " 57%|█████▋    | 172/300 [2:41:12<1:59:00, 55.79s/it]\n",
      " 58%|█████▊    | 173/300 [2:42:05<1:56:17, 54.94s/it]\n",
      " 58%|█████▊    | 174/300 [2:43:09<2:01:03, 57.65s/it]\n",
      " 58%|█████▊    | 175/300 [2:44:06<1:59:25, 57.33s/it]\n",
      " 59%|█████▊    | 176/300 [2:45:07<2:00:53, 58.49s/it]\n",
      " 59%|█████▉    | 177/300 [2:46:28<2:13:20, 65.05s/it]\n",
      " 59%|█████▉    | 178/300 [2:47:24<2:06:43, 62.32s/it]\n",
      " 60%|█████▉    | 179/300 [2:48:06<1:53:41, 56.38s/it]\n",
      " 60%|██████    | 180/300 [2:48:53<1:47:16, 53.63s/it]\n",
      "                                                     \n",
      "{'loss': 0.1267, 'grad_norm': 1.872575283050537, 'learning_rate': 8.316151202749142e-05, 'epoch': 5.46}\n",
      "\n",
      " 60%|██████    | 180/300 [2:48:53<1:47:16, 53.63s/it]\n",
      " 60%|██████    | 181/300 [2:49:57<1:52:11, 56.57s/it]\n",
      " 61%|██████    | 182/300 [2:51:24<2:09:33, 65.88s/it]\n",
      " 61%|██████    | 183/300 [2:52:16<2:00:18, 61.70s/it]\n",
      " 61%|██████▏   | 184/300 [2:53:17<1:58:52, 61.48s/it]\n",
      " 62%|██████▏   | 185/300 [2:53:55<1:44:03, 54.29s/it]\n",
      " 62%|██████▏   | 186/300 [2:54:53<1:45:42, 55.64s/it]\n",
      " 62%|██████▏   | 187/300 [2:55:40<1:39:23, 52.78s/it]\n",
      " 63%|██████▎   | 188/300 [2:56:25<1:34:25, 50.59s/it]\n",
      " 63%|██████▎   | 189/300 [2:57:41<1:47:53, 58.32s/it]\n",
      " 63%|██████▎   | 190/300 [2:58:33<1:43:03, 56.22s/it]\n",
      "                                                     \n",
      "{'loss': 0.1397, 'grad_norm': 2.1888365745544434, 'learning_rate': 7.628865979381443e-05, 'epoch': 5.77}\n",
      "\n",
      " 63%|██████▎   | 190/300 [2:58:33<1:43:03, 56.22s/it]\n",
      " 64%|██████▎   | 191/300 [2:59:26<1:40:25, 55.28s/it]\n",
      " 64%|██████▍   | 192/300 [3:00:16<1:36:45, 53.75s/it]\n",
      " 64%|██████▍   | 193/300 [3:01:22<1:42:13, 57.32s/it]\n",
      " 65%|██████▍   | 194/300 [3:02:30<1:47:10, 60.66s/it]\n",
      " 65%|██████▌   | 195/300 [3:03:35<1:48:36, 62.06s/it]\n",
      " 65%|██████▌   | 196/300 [3:04:38<1:47:58, 62.29s/it]\n",
      " 66%|██████▌   | 197/300 [3:05:18<1:35:30, 55.63s/it]\n",
      " 66%|██████▌   | 198/300 [3:05:40<1:17:04, 45.33s/it]\n",
      " 66%|██████▋   | 199/300 [3:06:55<1:31:21, 54.27s/it]\n",
      " 67%|██████▋   | 200/300 [3:08:03<1:37:39, 58.59s/it]\n",
      "                                                     \n",
      "{'loss': 0.121, 'grad_norm': 0.9411455392837524, 'learning_rate': 6.941580756013745e-05, 'epoch': 6.06}\n",
      "\n",
      " 67%|██████▋   | 200/300 [3:08:03<1:37:39, 58.59s/it]\n",
      " 67%|██████▋   | 201/300 [3:08:56<1:33:53, 56.90s/it]\n",
      " 67%|██████▋   | 202/300 [3:09:38<1:25:37, 52.42s/it]\n",
      " 68%|██████▊   | 203/300 [3:10:29<1:23:44, 51.80s/it]\n",
      " 68%|██████▊   | 204/300 [3:11:33<1:28:37, 55.39s/it]\n",
      " 68%|██████▊   | 205/300 [3:12:50<1:38:20, 62.11s/it]\n",
      " 69%|██████▊   | 206/300 [3:13:49<1:35:50, 61.17s/it]\n",
      " 69%|██████▉   | 207/300 [3:14:48<1:33:31, 60.34s/it]\n",
      " 69%|██████▉   | 208/300 [3:15:45<1:31:17, 59.53s/it]\n",
      " 70%|██████▉   | 209/300 [3:16:33<1:24:44, 55.87s/it]\n",
      " 70%|███████   | 210/300 [3:17:43<1:30:10, 60.11s/it]\n",
      "                                                     \n",
      "{'loss': 0.0898, 'grad_norm': 1.5152969360351562, 'learning_rate': 6.254295532646049e-05, 'epoch': 6.37}\n",
      "\n",
      " 70%|███████   | 210/300 [3:17:43<1:30:10, 60.11s/it]\n",
      " 70%|███████   | 211/300 [3:18:32<1:24:34, 57.01s/it]\n",
      " 71%|███████   | 212/300 [3:19:23<1:20:39, 54.99s/it]\n",
      " 71%|███████   | 213/300 [3:20:10<1:16:34, 52.81s/it]\n",
      " 71%|███████▏  | 214/300 [3:21:10<1:18:41, 54.90s/it]\n",
      " 72%|███████▏  | 215/300 [3:21:53<1:12:37, 51.26s/it]\n",
      " 72%|███████▏  | 216/300 [3:22:41<1:10:27, 50.33s/it]\n",
      " 72%|███████▏  | 217/300 [3:23:31<1:09:35, 50.30s/it]\n",
      " 73%|███████▎  | 218/300 [3:24:57<1:23:10, 60.86s/it]\n",
      " 73%|███████▎  | 219/300 [3:25:47<1:17:37, 57.51s/it]\n",
      " 73%|███████▎  | 220/300 [3:26:35<1:13:11, 54.89s/it]\n",
      "                                                     \n",
      "{'loss': 0.0944, 'grad_norm': 1.4966398477554321, 'learning_rate': 5.567010309278351e-05, 'epoch': 6.68}\n",
      "\n",
      " 73%|███████▎  | 220/300 [3:26:35<1:13:11, 54.89s/it]\n",
      " 74%|███████▎  | 221/300 [3:27:20<1:08:22, 51.93s/it]\n",
      " 74%|███████▍  | 222/300 [3:28:20<1:10:33, 54.28s/it]\n",
      " 74%|███████▍  | 223/300 [3:29:26<1:14:04, 57.72s/it]\n",
      " 75%|███████▍  | 224/300 [3:30:20<1:11:38, 56.56s/it]\n",
      " 75%|███████▌  | 225/300 [3:31:03<1:05:49, 52.67s/it]\n",
      " 75%|███████▌  | 226/300 [3:31:57<1:05:29, 53.10s/it]\n",
      " 76%|███████▌  | 227/300 [3:32:42<1:01:24, 50.47s/it]\n",
      " 76%|███████▌  | 228/300 [3:33:31<1:00:07, 50.10s/it]\n",
      " 76%|███████▋  | 229/300 [3:34:15<57:11, 48.33s/it]  \n",
      " 77%|███████▋  | 230/300 [3:35:12<59:23, 50.91s/it]\n",
      "                                                   \n",
      "{'loss': 0.0956, 'grad_norm': 1.444486141204834, 'learning_rate': 4.879725085910653e-05, 'epoch': 6.98}\n",
      "\n",
      " 77%|███████▋  | 230/300 [3:35:12<59:23, 50.91s/it]\n",
      " 77%|███████▋  | 231/300 [3:35:30<47:17, 41.12s/it]\n",
      " 77%|███████▋  | 232/300 [3:36:22<50:05, 44.20s/it]\n",
      " 78%|███████▊  | 233/300 [3:37:26<56:03, 50.20s/it]\n",
      " 78%|███████▊  | 234/300 [3:38:16<55:11, 50.18s/it]\n",
      " 78%|███████▊  | 235/300 [3:39:26<1:00:44, 56.07s/it]\n",
      " 79%|███████▊  | 236/300 [3:40:15<57:26, 53.85s/it]  \n",
      " 79%|███████▉  | 237/300 [3:41:16<58:52, 56.07s/it]\n",
      " 79%|███████▉  | 238/300 [3:42:06<56:10, 54.36s/it]\n",
      " 80%|███████▉  | 239/300 [3:42:58<54:31, 53.62s/it]\n",
      " 80%|████████  | 240/300 [3:43:53<53:54, 53.91s/it]\n",
      "                                                   \n",
      "{'loss': 0.0654, 'grad_norm': 0.7705118060112, 'learning_rate': 4.1924398625429554e-05, 'epoch': 7.28}\n",
      "\n",
      " 80%|████████  | 240/300 [3:43:53<53:54, 53.91s/it]\n",
      " 80%|████████  | 241/300 [3:44:51<54:17, 55.21s/it]\n",
      " 81%|████████  | 242/300 [3:46:01<57:47, 59.78s/it]\n",
      " 81%|████████  | 243/300 [3:46:53<54:28, 57.35s/it]\n",
      " 81%|████████▏ | 244/300 [3:47:49<53:09, 56.96s/it]\n",
      " 82%|████████▏ | 245/300 [3:48:47<52:32, 57.31s/it]\n",
      " 82%|████████▏ | 246/300 [3:49:48<52:35, 58.44s/it]\n",
      " 82%|████████▏ | 247/300 [3:50:39<49:27, 56.00s/it]\n",
      " 83%|████████▎ | 248/300 [3:51:21<44:53, 51.80s/it]\n",
      " 83%|████████▎ | 249/300 [3:51:59<40:34, 47.74s/it]\n",
      " 83%|████████▎ | 250/300 [3:52:46<39:38, 47.57s/it]\n",
      "                                                   \n",
      "{'loss': 0.0655, 'grad_norm': 0.9886437654495239, 'learning_rate': 3.5051546391752576e-05, 'epoch': 7.58}\n",
      "\n",
      " 83%|████████▎ | 250/300 [3:52:46<39:38, 47.57s/it]\n",
      " 84%|████████▎ | 251/300 [3:53:40<40:25, 49.49s/it]\n",
      " 84%|████████▍ | 252/300 [3:54:42<42:37, 53.29s/it]\n",
      " 84%|████████▍ | 253/300 [3:55:32<40:53, 52.20s/it]\n",
      " 85%|████████▍ | 254/300 [3:56:26<40:32, 52.89s/it]\n",
      " 85%|████████▌ | 255/300 [3:57:31<42:12, 56.28s/it]\n",
      " 85%|████████▌ | 256/300 [3:58:34<42:56, 58.57s/it]\n",
      " 86%|████████▌ | 257/300 [3:59:28<40:56, 57.13s/it]\n",
      " 86%|████████▌ | 258/300 [4:00:21<38:59, 55.69s/it]\n",
      " 86%|████████▋ | 259/300 [4:01:18<38:23, 56.18s/it]\n",
      " 87%|████████▋ | 260/300 [4:02:11<36:50, 55.25s/it]\n",
      "                                                   \n",
      "{'loss': 0.0586, 'grad_norm': 1.3176240921020508, 'learning_rate': 2.81786941580756e-05, 'epoch': 7.89}\n",
      "\n",
      " 87%|████████▋ | 260/300 [4:02:11<36:50, 55.25s/it]\n",
      " 87%|████████▋ | 261/300 [4:02:55<33:41, 51.82s/it]\n",
      " 87%|████████▋ | 262/300 [4:03:50<33:29, 52.89s/it]\n",
      " 88%|████████▊ | 263/300 [4:04:34<30:59, 50.24s/it]\n",
      " 88%|████████▊ | 264/300 [4:05:23<29:48, 49.69s/it]\n",
      " 88%|████████▊ | 265/300 [4:06:12<28:51, 49.48s/it]\n",
      " 89%|████████▊ | 266/300 [4:07:00<27:54, 49.25s/it]\n",
      " 89%|████████▉ | 267/300 [4:07:57<28:14, 51.34s/it]\n",
      " 89%|████████▉ | 268/300 [4:08:45<26:58, 50.58s/it]\n",
      " 90%|████████▉ | 269/300 [4:09:43<27:11, 52.62s/it]\n",
      " 90%|█████████ | 270/300 [4:10:22<24:18, 48.61s/it]\n",
      "                                                   \n",
      "{'loss': 0.059, 'grad_norm': 0.9445586800575256, 'learning_rate': 2.1305841924398627e-05, 'epoch': 8.18}\n",
      "\n",
      " 90%|█████████ | 270/300 [4:10:22<24:18, 48.61s/it]\n",
      " 90%|█████████ | 271/300 [4:11:16<24:12, 50.09s/it]\n",
      " 91%|█████████ | 272/300 [4:12:17<24:57, 53.48s/it]\n",
      " 91%|█████████ | 273/300 [4:13:18<25:05, 55.75s/it]\n",
      " 91%|█████████▏| 274/300 [4:14:13<24:04, 55.54s/it]\n",
      " 92%|█████████▏| 275/300 [4:15:15<23:53, 57.35s/it]\n",
      " 92%|█████████▏| 276/300 [4:16:11<22:46, 56.95s/it]\n",
      " 92%|█████████▏| 277/300 [4:17:07<21:47, 56.84s/it]\n",
      " 93%|█████████▎| 278/300 [4:17:58<20:10, 55.04s/it]\n",
      " 93%|█████████▎| 279/300 [4:19:01<20:04, 57.37s/it]\n",
      " 93%|█████████▎| 280/300 [4:20:07<20:01, 60.09s/it]\n",
      "                                                   \n",
      "{'loss': 0.0448, 'grad_norm': 0.6323841214179993, 'learning_rate': 1.4432989690721649e-05, 'epoch': 8.49}\n",
      "\n",
      " 93%|█████████▎| 280/300 [4:20:07<20:01, 60.09s/it]\n",
      " 94%|█████████▎| 281/300 [4:20:53<17:37, 55.68s/it]\n",
      " 94%|█████████▍| 282/300 [4:22:03<17:58, 59.93s/it]\n",
      " 94%|█████████▍| 283/300 [4:22:55<16:23, 57.83s/it]\n",
      " 95%|█████████▍| 284/300 [4:23:46<14:50, 55.67s/it]\n",
      " 95%|█████████▌| 285/300 [4:24:56<14:59, 59.98s/it]\n",
      " 95%|█████████▌| 286/300 [4:25:47<13:21, 57.24s/it]\n",
      " 96%|█████████▌| 287/300 [4:26:35<11:48, 54.52s/it]\n",
      " 96%|█████████▌| 288/300 [4:27:23<10:29, 52.49s/it]\n",
      " 96%|█████████▋| 289/300 [4:28:27<10:16, 56.06s/it]\n",
      " 97%|█████████▋| 290/300 [4:29:44<10:22, 62.23s/it]\n",
      "                                                   \n",
      "{'loss': 0.0531, 'grad_norm': 0.801028311252594, 'learning_rate': 7.560137457044673e-06, 'epoch': 8.8}\n",
      "\n",
      " 97%|█████████▋| 290/300 [4:29:44<10:22, 62.23s/it]\n",
      " 97%|█████████▋| 291/300 [4:30:41<09:05, 60.59s/it]\n",
      " 97%|█████████▋| 292/300 [4:31:52<08:31, 63.90s/it]\n",
      " 98%|█████████▊| 293/300 [4:32:41<06:55, 59.39s/it]\n",
      " 98%|█████████▊| 294/300 [4:33:56<06:23, 63.99s/it]\n",
      " 98%|█████████▊| 295/300 [4:34:34<04:41, 56.38s/it]\n",
      " 99%|█████████▊| 296/300 [4:35:24<03:37, 54.27s/it]\n",
      " 99%|█████████▉| 297/300 [4:35:57<02:24, 48.08s/it]\n",
      " 99%|█████████▉| 298/300 [4:36:47<01:37, 48.53s/it]\n",
      "100%|█████████▉| 299/300 [4:37:45<00:51, 51.49s/it]\n",
      "100%|██████████| 300/300 [4:38:51<00:00, 55.64s/it]\n",
      "                                                   \n",
      "{'loss': 0.0492, 'grad_norm': 0.7283040285110474, 'learning_rate': 6.872852233676976e-07, 'epoch': 9.09}\n",
      "\n",
      "100%|██████████| 300/300 [4:38:51<00:00, 55.64s/it]\n",
      "                                                   \n",
      "{'train_runtime': 16732.5209, 'train_samples_per_second': 0.072, 'train_steps_per_second': 0.018, 'train_loss': 0.48840298304955165, 'epoch': 9.09}\n",
      "\n",
      "100%|██████████| 300/300 [4:38:52<00:00, 55.64s/it]\n",
      "100%|██████████| 300/300 [4:38:52<00:00, 55.78s/it]\n",
      "[OK] LoRA adapter saved to: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17\\adapter\n",
      "\n",
      "[proc exit code] 0\n"
     ]
    }
   ],
   "source": [
    "# ===== G) Launch training (strictly offline) =====\n",
    "import subprocess, sys, shlex\n",
    "\n",
    "args = [\n",
    "    sys.executable, str(ROOT / \"src/train/cpu_lora_hf.py\"),\n",
    "    \"--model_id\", MODEL_ID,                      # local folder\n",
    "    \"--train_jsonl\", str(ALPACA),\n",
    "    \"--out_dir\", str(OUT),\n",
    "    \"--max_steps\", str(MAX_STEPS),\n",
    "    \"--batch_size\", str(BATCH_SIZE),\n",
    "    \"--grad_accum\", str(GRAD_ACCUM),\n",
    "    \"--lr\", str(LR),\n",
    "    \"--max_seq_len\", str(MAX_SEQ_LEN),\n",
    "    \"--num_threads\", str(NUM_THREADS),\n",
    "    \"--lora_r\", str(LORA_R),\n",
    "    \"--lora_alpha\", str(LORA_ALPHA),\n",
    "    \"--lora_dropout\", str(LORA_DROPOUT),\n",
    "    \"--seed\", str(SEED),\n",
    "    \"--local_files_only\",\n",
    "    \"--trust_remote_code\",\n",
    "]\n",
    "print(\"Launching:\", \" \".join(map(shlex.quote, args)))\n",
    "proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "for line in proc.stdout:\n",
    "    print(line, end=\"\")\n",
    "code = proc.wait()\n",
    "print(\"\\n[proc exit code]\", code)\n",
    "if code != 0:\n",
    "    raise RuntimeError(\"Training failed; see logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cff75e",
   "metadata": {},
   "source": [
    "### **Inspect saved adapter files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42de4bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter dir: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17\\adapter | exists: True\n",
      "['adapter_config.json', 'adapter_model.safetensors', 'added_tokens.json', 'chat_template.jinja', 'merges.txt', 'README.md', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "# ===== H) Inspect adapter folder =====\n",
    "adapter_dir = OUT / \"adapter\"\n",
    "print(\"Adapter dir:\", adapter_dir, \"| exists:\", adapter_dir.exists())\n",
    "if adapter_dir.exists():\n",
    "    print([p.name for p in adapter_dir.iterdir()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34739ef1",
   "metadata": {},
   "source": [
    "### **Merging LoRA into base for single-file inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: 'd:\\Anaconda\\envs\\pdf-agent-2\\python.exe' 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\src\\train\\merge_lora.py' --base_model 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\models\\Qwen2.5-1.5B-Instruct' --lora_dir 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17\\adapter' --out_dir 'D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17_merged' --local_files_only --trust_remote_code\n",
      "[OK] merged model saved to: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17_merged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, shlex\n",
    "MERGED_DIR = ROOT / \"outputs\" / \"lora_hf\" / \"title17_merged\"\n",
    "args = [\n",
    "    sys.executable, str(ROOT / \"src/train/merge_lora.py\"),\n",
    "    \"--base_model\", MODEL_ID,                   # local base directory\n",
    "    \"--lora_dir\", str(OUT / \"adapter\"),\n",
    "    \"--out_dir\", str(MERGED_DIR),\n",
    "    \"--local_files_only\",\n",
    "    \"--trust_remote_code\",\n",
    "]\n",
    "print(\"Merging:\", \" \".join(map(shlex.quote, args)))\n",
    "print(subprocess.run(args, capture_output=True, text=True).stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a3a4e",
   "metadata": {},
   "source": [
    "### **Inference with *adapter* (no merge)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5185d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize § 114 and note the performance right caveat. End with [pp. 67–88]. ### Section 114: Scope of exclusive rights: Secondary transmissions by cable of local television programming\n",
      "#### (b) Performance for secondary transmission by cable.-\n",
      "-  *(1)* The exclusive rights granted by this section shall not apply to a performance made by a person other than a natural person described in paragraph (1)(C), engaged in a secondary transmission by cable of local television programming.\n",
      "- *(2)* In addition, no person shall engage in a secondary transmission by cable of local television programming that is made without authorization under this section or section 106 of the copyright Act [see footnote 2 for source], except as provided in subparagraph (D).*\n",
      "- *(3)* A secondary transmission by cable of local television programming is a transmission of a performance embodied in a primary transmission of a broadcast station to the private home of a subscriber residing outside the local service area of such broadcast\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_path = MODEL_ID\n",
    "adapter_dir = OUT / \"adapter\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_path, local_files_only=True, trust_remote_code=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\": \"cpu\"},\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, str(adapter_dir), local_files_only=True)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Summarize § 114 and note the performance right caveat. End with [pp. 67–88].\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=180, do_sample=False)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44f1e9",
   "metadata": {},
   "source": [
    "### **Inference with *merged* model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1346b1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d9227eb586446ba341f124fd357e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give 3 bullets on § 1201 anticircumvention. End with [pp. 314–317]. The anticyclus was designed to protect intellectual property rights by prohibiting counterfeiting and infringement actions against domestic products that comply with relevant laws and regulations.\n",
      "The anticyclus has undergone several amendments, including the Berne Convention Implementation Act (1988), the Architectural Works Copyright Protection Act (1990), the Anticounterfeiting Consumer Product Act (1996), the Online Copyright Infringement Liability Limitation Act (2000), the Online Illegal Activities Elimination Act (2005), and the Prioritizing Resources to Combat Counterfeit Goods Act of 2008 (2008).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MERGED_DIR = ROOT / \"outputs\" / \"lora_hf\" / \"title17_merged\"\n",
    "tok = AutoTokenizer.from_pretrained(str(MERGED_DIR), local_files_only=True, trust_remote_code=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MERGED_DIR),\n",
    "    torch_dtype=torch.float32,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\": \"cpu\"},\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Give 3 bullets on § 1201 anticircumvention. End with [pp. 314–317].\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=180, do_sample=False)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dc46f",
   "metadata": {},
   "source": [
    "### **Pinning a small README next to the adapter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607a6c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17\\README.txt\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "readme = OUT / \"README.txt\"\n",
    "readme.write_text(\n",
    "    f\"\"\"Title 17 LoRA (CPU, PEFT)\n",
    "Model: {MODEL_ID}\n",
    "Created: {datetime.now().isoformat()}\n",
    "Cmd: cpu_lora_hf.py --model_id \"{MODEL_ID}\" --train_jsonl \"{ALPACA}\" --out_dir \"{OUT}\"\n",
    "     --max_steps {MAX_STEPS} --batch_size {BATCH_SIZE} --grad_accum {GRAD_ACCUM}\n",
    "     --lr {LR} --max_seq_len {MAX_SEQ_LEN} --num_threads {NUM_THREADS}\n",
    "     --lora_r {LORA_R} --lora_alpha {LORA_ALPHA} --lora_dropout {LORA_DROPOUT} --seed {SEED}\n",
    "\"\"\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Wrote {readme}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-agent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
