{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e107a5",
   "metadata": {},
   "source": [
    "### **Setup & paths (strictly offline)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT     : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\n",
      "SFT_DIR  : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\data\\sft\n",
      "CHUNKS   : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\data\\chunks\n",
      "GRAPH    : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\graph\\graph\n",
      "RERANKER : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\reranker\\title17\n",
      "Qwen     : D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora_hf\\title17_merged\n",
      "Python   : 3.11.13 | CPU threads: 8\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, os, platform, json, re\n",
    "\n",
    "CWD  = Path.cwd().resolve()\n",
    "ROOT = CWD if (CWD / \"src\").exists() else CWD.parent\n",
    "if str(ROOT) not in sys.path: sys.path.append(str(ROOT))\n",
    "\n",
    "DATA       = ROOT / \"data\"\n",
    "SFT_DIR    = DATA / \"sft\"\n",
    "CHUNKS_DIR = DATA / \"chunks\"\n",
    "GRAPH_DIR  = ROOT / \"outputs\" / \"graph\" / \"graph\"\n",
    "RERANK_DIR = ROOT / \"outputs\" / \"reranker\" / \"title17\"\n",
    "MERGED_QWEN= ROOT / \"outputs\" / \"lora_hf\" / \"title17_merged\"\n",
    "OUT_DIR    = ROOT / \"outputs\" / \"eval\" / \"ragas\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"ROOT     :\", ROOT)\n",
    "print(\"SFT_DIR  :\", SFT_DIR)\n",
    "print(\"CHUNKS   :\", CHUNKS_DIR)\n",
    "print(\"GRAPH    :\", GRAPH_DIR)\n",
    "print(\"RERANKER :\", RERANK_DIR)\n",
    "print(\"Qwen     :\", MERGED_QWEN)\n",
    "print(\"Python   :\", platform.python_version(), \"| CPU threads:\", os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc147ad8",
   "metadata": {},
   "source": [
    "### **Hierarchical retriever (BM25 -> CE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ff7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pdf-agent-2\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1  score=0.507  node=SEC-00017  chunk=title17-h-342  pages=[]\n",
      "§ 114 · Scope of exclusive rights in sound recordings 48\n",
      "\n",
      "#2  score=-0.890  node=SEC-00019  chunk=title17-h-689  pages=[]\n",
      "§ 116 · Negotiated licenses for public performances by means of coin-operated phonorecord players 53\n",
      "\n",
      "#3  score=-1.469  node=SEC-00016  chunk=title17-h-328  pages=[]\n",
      "§ 113 · Scope of exclusive rights in pictorial, graphic, and sculptural works 47\n",
      "\n",
      "#4  score=-1.862  node=SEC-00015  chunk=title17-h-307  pages=[]\n",
      "§ 112 · Limitations on exclusive rights: Ephemeral recordings 46\n",
      "\n",
      "#5  score=-1.915  node=SEC-00013  chunk=title17-h-220  pages=[]\n",
      "§ 110 · Limitations on exclusive rights: Exemption of certain performances and displays 43\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "hier = json.loads((GRAPH_DIR / \"hierarchy.json\").read_text(encoding=\"utf-8\"))\n",
    "nodes = hier[\"nodes\"]\n",
    "\n",
    "node_records = [json.loads(l) for l in open(GRAPH_DIR / \"node_texts.jsonl\", \"r\", encoding=\"utf-8\")]\n",
    "node_text_by_id = {r[\"node_id\"]: r[\"text\"] for r in node_records}\n",
    "node_name_by_id = {r[\"node_id\"]: r[\"name\"] for r in node_records}\n",
    "\n",
    "chunk_text   = {}\n",
    "chunk_pages  = {}\n",
    "chunk_sec    = {}\n",
    "for fp in sorted(CHUNKS_DIR.glob(\"*.jsonl\")):\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            cid = obj.get(\"id\") or obj.get(\"chunk_id\")\n",
    "            if not cid: \n",
    "                continue\n",
    "            txt = obj.get(\"text\") or obj.get(\"content\") or \"\"\n",
    "            if not txt.strip(): \n",
    "                continue\n",
    "            chunk_text[cid]  = txt\n",
    "            chunk_pages[cid] = obj.get(\"pages\") or []\n",
    "            chunk_sec[cid]   = obj.get(\"section\") or \"\"\n",
    "\n",
    "node2chunks = {}\n",
    "for nd in nodes:\n",
    "    nid = nd[\"id\"]\n",
    "    node2chunks[nid] = nd.get(\"chunk_ids\") or []\n",
    "\n",
    "def tok(s:str):\n",
    "    return re.findall(r\"[A-Za-z0-9_]+\", (s or \"\").lower())\n",
    "\n",
    "node_ids  = [r[\"node_id\"] for r in node_records]\n",
    "node_texts= [node_text_by_id[nid] for nid in node_ids]\n",
    "bm25_nodes= BM25Okapi([tok(t) for t in node_texts])\n",
    "\n",
    "reranker = CrossEncoder(str(RERANK_DIR), device=\"cpu\")\n",
    "\n",
    "def search_hier(\n",
    "    query: str, \n",
    "    k_nodes: int = 40, \n",
    "    k_final_nodes: int = 6, \n",
    "    k_each_node: int = 12, \n",
    "    k_final_chunks: int = 8\n",
    "):\n",
    "    scores = bm25_nodes.get_scores(tok(query))\n",
    "    cand_node_idx = sorted(range(len(node_ids)), key=lambda i: scores[i], reverse=True)[:k_nodes]\n",
    "    cand_nodes = [(node_ids[i], node_texts[i]) for i in cand_node_idx]\n",
    "\n",
    "    ce_node_scores = reranker.predict([[query, t] for _, t in cand_nodes])\n",
    "    ranked_nodes = [cand_nodes[i] for i in sorted(range(len(cand_nodes)), key=lambda j: ce_node_scores[j], reverse=True)[:k_final_nodes]]\n",
    "    ranked_node_ids = [nid for nid,_ in ranked_nodes]\n",
    "\n",
    "    final_cands = []\n",
    "    for nid in ranked_node_ids:\n",
    "        cids = [cid for cid in node2chunks.get(nid, []) if cid in chunk_text]\n",
    "        if not cids:\n",
    "            continue\n",
    "        bm25_local = BM25Okapi([tok(chunk_text[c]) for c in cids])\n",
    "        local_scores = bm25_local.get_scores(tok(query))\n",
    "        local_idx = sorted(range(len(cids)), key=lambda i: local_scores[i], reverse=True)[:k_each_node]\n",
    "        for i in local_idx:\n",
    "            cid = cids[i]\n",
    "            final_cands.append({\n",
    "                \"chunk_id\": cid,\n",
    "                \"node_id\": nid,\n",
    "                \"node_name\": node_name_by_id.get(nid, nid),\n",
    "                \"text\": chunk_text[cid],\n",
    "                \"pages\": chunk_pages.get(cid) or [],\n",
    "                \"section\": chunk_sec.get(cid) or \"\",\n",
    "            })\n",
    "\n",
    "    if not final_cands:\n",
    "        return []\n",
    "\n",
    "    ce_scores = reranker.predict([[query, c[\"text\"]] for c in final_cands])\n",
    "    for c, s in zip(final_cands, ce_scores):\n",
    "        c[\"score\"] = float(s)\n",
    "    final = sorted(final_cands, key=lambda x: x[\"score\"], reverse=True)[:k_final_chunks]\n",
    "    return final\n",
    "\n",
    "q = \"Summarize § 114 performance rights caveat. End with [pp. 67–88].\"\n",
    "hits = search_hier(q, k_nodes=40, k_final_nodes=6, k_each_node=12, k_final_chunks=5)\n",
    "for i, h in enumerate(hits, 1):\n",
    "    snip = (h[\"text\"][:600] + \"…\") if len(h[\"text\"]) > 600 else h[\"text\"]\n",
    "    print(f\"\\n#{i}  score={h['score']:.3f}  node={h['node_id']}  chunk={h['chunk_id']}  pages={h['pages']}\\n{snip}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52310fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cd108ba8314d8aac311b126d87ee65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(str(MERGED_QWEN), local_files_only=True, trust_remote_code=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MERGED_QWEN),\n",
    "    torch_dtype=torch.float32,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\": \"cpu\"},\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You answer strictly using the provided CONTEXTS. \"\n",
    "    \"Cite page numbers if present. If the answer is not in the contexts, say you don't know.\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, contexts: list[str]) -> str:\n",
    "    ctx_block = \"\\n\\n\".join([f\"[CTX {i+1}]\\n{c}\" for i, c in enumerate(contexts)])\n",
    "    return (\n",
    "        f\"System: {SYS_PROMPT}\\n\\n\"\n",
    "        f\"CONTEXTS:\\n{ctx_block}\\n\\n\"\n",
    "        f\"User: {question}\\nAssistant:\"\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def answer_with_contexts(question: str, contexts: list[str], max_new_tokens=220) -> str:\n",
    "    prompt = build_prompt(question, contexts)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    return tok.decode(out[0], skip_special_tokens=True).split(\"Assistant:\", 1)[-1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade375b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval examples: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Q: Summarize the section:\n",
      "Heading: Copyright Law United States Copyri > (E) Musical works database .-\n",
      "Summarize as 12–15 de...\n",
      "Answer: - The mechanical licensing collective is responsible for establishing and maintaining a comprehensive database of musical works, including information on shares, copyright owners, and sound recordings ...\n",
      "GT   : - - The mechanical licensing collective must establish and maintain a database containing information about musical works, shares of such works, copyright owners, and sound recordings.\n",
      "- - The collect ...\n",
      "#ctx : 5\n",
      "\n",
      "[2] Q: Summarize the section:\n",
      "Heading: Copyright Law United States Copyri > (g) Encryption Research. -\n",
      "Give a brief 2–3 sentenc...\n",
      "Answer: The passage outlines provisions related to encryption research and its distribution under certain conditions. Noncommercial distributions of unpublished works by researchers are exempt if: The work wa ...\n",
      "GT   : Encryption research refers to activities that identify and analyze flaws in encryption technologies applied to copyrighted works. A person may circumvent a technological measure for good faith encrypt ...\n",
      "#ctx : 5\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "TEST_JSONL = SFT_DIR / \"test.jsonl\"\n",
    "assert TEST_JSONL.exists(), f\"Missing {TEST_JSONL}\"\n",
    "\n",
    "def extract_q_gt(row: dict) -> tuple[str|None, str|None]:\n",
    "    msgs = row.get(\"messages\")\n",
    "    if isinstance(msgs, list):\n",
    "        q = None\n",
    "        for m in msgs:\n",
    "            if (m.get(\"role\") or \"\").lower() == \"user\":\n",
    "                q = (m.get(\"content\") or \"\").strip()\n",
    "        gt = (row.get(\"response\") or \"\").strip()\n",
    "        return (q, gt) if q else (None, None)\n",
    "    q = (row.get(\"instruction\") or row.get(\"question\") or \"\").strip()\n",
    "    gt = (row.get(\"response\") or row.get(\"answer\") or \"\").strip()\n",
    "    return (q if q else None, gt if gt else None)\n",
    "\n",
    "rows = []\n",
    "with open(TEST_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        q, gt = extract_q_gt(obj)\n",
    "        if q and gt and len(q) > 12:\n",
    "            rows.append((q, gt))\n",
    "\n",
    "N = min(12, len(rows))\n",
    "sample = rows[:N]\n",
    "print(f\"Eval examples: {len(sample)}\")\n",
    "\n",
    "eval_records = []\n",
    "for q, gt in sample:\n",
    "    hits = search_hier(q, k_nodes=40, k_final_nodes=6, k_each_node=12, k_final_chunks=5)\n",
    "    contexts = [h[\"text\"] for h in hits]\n",
    "    ans = answer_with_contexts(q, contexts)\n",
    "    eval_records.append({\n",
    "        \"question\": q,\n",
    "        \"contexts\": contexts,\n",
    "        \"answer\": ans,\n",
    "        \"ground_truth\": gt,\n",
    "    })\n",
    "\n",
    "for i, r in enumerate(eval_records[:2], 1):\n",
    "    print(f\"\\n[{i}] Q: {r['question'][:120]}...\")\n",
    "    print(\"Answer:\", r[\"answer\"][:200], \"...\")\n",
    "    print(\"GT   :\", r[\"ground_truth\"][:200], \"...\")\n",
    "    print(\"#ctx :\", len(r[\"contexts\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da2df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] judge LLM pipeline ready (CPU)\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "def _pip_install(pkg: str):\n",
    "    print(subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
    "                         capture_output=True, text=True).stdout)\n",
    "\n",
    "try:\n",
    "    _pip_install(\"langchain-huggingface>=0.1.0\")\n",
    "    from langchain_huggingface import HuggingFacePipeline as HF_PIPELINE_WRAPPER\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain_community.llms import HuggingFacePipeline as HF_PIPELINE_WRAPPER\n",
    "    except Exception:\n",
    "        _pip_install(\"langchain-community==0.3.2\")\n",
    "        from langchain_community.llms import HuggingFacePipeline as HF_PIPELINE_WRAPPER\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tok.eos_token_id,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "judge_llm = HF_PIPELINE_WRAPPER(pipeline=gen_pipe)\n",
    "print(\"[OK] judge LLM pipeline ready (CPU)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] generation_config pinned\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "try:\n",
    "    model.generation_config.do_sample = False\n",
    "    model.generation_config.top_p = 1.0\n",
    "    model.generation_config.temperature = 0.0\n",
    "    model.generation_config.num_beams = 1\n",
    "    print(\"[OK] generation_config pinned\")\n",
    "except Exception as e:\n",
    "    print(\"[warn] couldn't pin generation_config:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e12e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using offline hashed embeddings (no OpenAI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[0]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[2]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, json, re\n",
    "os.environ[\"RAGAS_DISABLE_TELEMETRY\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def _pip_install(pkg: str):\n",
    "    print(subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
    "                         capture_output=True, text=True).stdout)\n",
    "\n",
    "# --- deps\n",
    "try:\n",
    "    import ragas\n",
    "except Exception:\n",
    "    _pip_install(\"ragas==0.1.9\")\n",
    "    import ragas\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_precision, context_recall\n",
    "\n",
    "# ---- embeddings\n",
    "embeddings = None\n",
    "LOCAL_EMB = ROOT / \"models\" / \"all-MiniLM-L6-v2\"\n",
    "try:\n",
    "    if LOCAL_EMB.exists():\n",
    "        try:\n",
    "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        except Exception:\n",
    "            _pip_install(\"langchain-community==0.3.2\")\n",
    "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=str(LOCAL_EMB))\n",
    "        print(\"[info] using local embeddings:\", LOCAL_EMB)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    import numpy as np\n",
    "    from ragas.embeddings.base import BaseRagasEmbeddings\n",
    "    class LocalHashEmbeddings(BaseRagasEmbeddings):\n",
    "        def __init__(self, dim: int = 512):\n",
    "            self.dim = dim\n",
    "            self._rx = re.compile(r\"[A-Za-z0-9_]+\")\n",
    "        def _v(self, text: str):\n",
    "            v = np.zeros(self.dim, dtype=np.float32)\n",
    "            for t in self._rx.findall((text or \"\").lower()):\n",
    "                v[hash(t) % self.dim] += 1.0\n",
    "            n = np.linalg.norm(v)\n",
    "            if n > 0: v /= n\n",
    "            return v.astype(np.float32).tolist()\n",
    "        def embed_documents(self, texts): return [self._v(t) for t in texts]\n",
    "        def embed_query(self, text):      return self._v(text)\n",
    "        async def aembed_documents(self, texts): return self.embed_documents(texts)\n",
    "        async def aembed_query(self, text):      return self.embed_query(text)\n",
    "    embeddings = LocalHashEmbeddings(dim=512)\n",
    "    print(\"[info] using offline hashed embeddings (no OpenAI)\")\n",
    "\n",
    "assert \"eval_records\" in globals() and len(eval_records) > 0, \"Run the earlier cells that build eval_records.\"\n",
    "ds = Dataset.from_list(eval_records)\n",
    "\n",
    "metrics = [context_precision, context_recall]\n",
    "\n",
    "try:\n",
    "    from ragas.run_config import RunConfig\n",
    "except Exception:\n",
    "    try:\n",
    "        from ragas.config import RunConfig\n",
    "    except Exception:\n",
    "        class RunConfig:\n",
    "            def __init__(self, timeout=None, max_workers=None):\n",
    "                self.timeout = timeout\n",
    "                self.max_workers = max_workers\n",
    "\n",
    "rc = RunConfig(timeout=600, max_workers=1)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=metrics,\n",
    "    embeddings=embeddings,\n",
    "    run_config=rc,\n",
    "    batch_size=4,\n",
    "    show_progress=False,\n",
    "    raise_exceptions=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    _pip_install(\"pandas>=2.0.0\")\n",
    "    import pandas as pd\n",
    "\n",
    "df = result.to_pandas()\n",
    "\n",
    "import numpy as np\n",
    "def _norm(v): \n",
    "    v = np.asarray(v, dtype=np.float32); \n",
    "    n = np.linalg.norm(v); \n",
    "    return v if n == 0 else v / n\n",
    "\n",
    "def _cos(a, b): \n",
    "    a = _norm(a); b = _norm(b); \n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "ans_sim = []\n",
    "for rec in eval_records:\n",
    "    a = rec.get(\"answer\") or \"\"\n",
    "    ctx = \"\\n\\n\".join(rec.get(\"contexts\") or [])\n",
    "    va = embeddings.embed_query(a)\n",
    "    vc = embeddings.embed_query(ctx)\n",
    "    ans_sim.append(_cos(va, vc))\n",
    "\n",
    "df[\"answer_similarity\"] = ans_sim\n",
    "\n",
    "# Aggregate means\n",
    "agg = {}\n",
    "for col in [\"context_precision\", \"context_recall\", \"answer_similarity\"]:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            agg[col] = float(pd.to_numeric(df[col], errors=\"coerce\").mean())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"\\n=== RAG (embedding-only) ===\")\n",
    "for k, v in agg.items():\n",
    "    print(f\"{k:>18s}: {v:.4f}\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_DIR / \"ragas_details.csv\", index=False, encoding=\"utf-8\")\n",
    "(OUT_DIR / \"ragas_report.json\").write_text(json.dumps(agg, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\n[OK] wrote\", OUT_DIR / \"ragas_report.json\")\n",
    "print(\"[OK] wrote\", OUT_DIR / \"ragas_details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49581e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-agent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
