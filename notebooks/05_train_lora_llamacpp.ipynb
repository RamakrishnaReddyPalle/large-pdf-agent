{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef9a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "ROOT = Path.cwd().resolve() if (Path.cwd()/\"src\").exists() else Path.cwd().resolve().parent\n",
    "if str(ROOT) not in sys.path: sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312d99a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote 130 alpaca examples → D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\data\\sft\\alpaca.train.jsonl\n"
     ]
    }
   ],
   "source": [
    "from src.train.sft_to_alpaca import to_alpaca\n",
    "to_alpaca(ROOT/\"data/sft/train.jsonl\", ROOT/\"data/sft/alpaca.train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ceb336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\n",
      "ALPACA exists? True\n",
      "BASE_GGUF exists? True\n",
      "OUT: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\outputs\\lora\n"
     ]
    }
   ],
   "source": [
    "# 05_train_lora_llamacpp.ipynb — Cell 1: setup\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "MODELS = ROOT / \"models\"\n",
    "OUT = ROOT / \"outputs\" / \"lora\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ALPACA = DATA / \"sft\" / \"alpaca.train.jsonl\"\n",
    "\n",
    "# PICK YOUR GGUF:\n",
    "BASE_GGUF = MODELS / \"llama-3.1-8b-instruct-f16.gguf\"   # change if you use a smaller GGUF\n",
    "# Example small alternative:\n",
    "# BASE_GGUF = MODELS / \"TinyLlama-1.1B-Chat-v1.0.Q5_K_M.gguf\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"ALPACA exists?\", ALPACA.exists())\n",
    "print(\"BASE_GGUF exists?\", BASE_GGUF.exists())\n",
    "print(\"OUT:\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 2: make a tiny subset for faster CPU LoRA (optional)\n",
    "# SUBSET = DATA / \"sft\" / \"alpaca.train.small.jsonl\"\n",
    "# keep = 200  # adjust down if CPU is struggling\n",
    "\n",
    "# cnt = 0\n",
    "# with open(SUBSET, \"w\", encoding=\"utf-8\") as w, open(ALPACA, \"r\", encoding=\"utf-8\") as r:\n",
    "#     for line in r:\n",
    "#         w.write(line)\n",
    "#         cnt += 1\n",
    "#         if cnt >= keep:\n",
    "#             break\n",
    "\n",
    "# print(\"[OK] wrote subset:\", SUBSET, \"| rows:\", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86102882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: generate a PowerShell script to launch CPU LoRA finetune\n",
    "LLAMACPP_DIR = ROOT / \"llama.cpp\"            # adjust if your llama.cpp lives elsewhere\n",
    "FINETUNE_EXE = LLAMACPP_DIR / \"finetune.exe\" # or \"llama-finetune.exe\" depending on your build\n",
    "\n",
    "ADAPTER_OUT = OUT / \"title17-lora.gguf\"\n",
    "\n",
    "ps = f\"\"\"\n",
    "# PowerShell: CPU LoRA finetune with llama.cpp\n",
    "$ErrorActionPreference = \"Stop\"\n",
    "\n",
    "$exe = \"{FINETUNE_EXE.as_posix()}\"\n",
    "if (!(Test-Path $exe)) {{\n",
    "  Write-Error \"finetune.exe not found at $exe — build llama.cpp first.\"\n",
    "}}\n",
    "\n",
    "# Pick dataset file: small subset for CPU or the full\n",
    "$train = \"{(DATA / 'sft' / 'alpaca.train.small.jsonl').as_posix()}\"\n",
    "# $train = \"{ALPACA.as_posix()}\"  # <- use this if you want full set (slower)\n",
    "\n",
    "# Params tuned for CPU feasibility\n",
    "& $exe `\n",
    "  --model \"{BASE_GGUF.as_posix()}\" `\n",
    "  --train-data \"$train\" `\n",
    "  --out-lora \"{ADAPTER_OUT.as_posix()}\" `\n",
    "  --lora-r 8 `\n",
    "  --lora-alpha 16 `\n",
    "  --epochs 1 `\n",
    "  --batch 1 `\n",
    "  --seq-len 512 `\n",
    "  --threads 8\n",
    "\n",
    "if ($LASTEXITCODE -ne 0) {{ exit $LASTEXITCODE }}\n",
    "Write-Host \"[OK] LoRA saved to {ADAPTER_OUT.as_posix()}\"\n",
    "\"\"\"\n",
    "\n",
    "PS1 = ROOT / \"src\" / \"train\" / \"llamacpp_finetune.ps1\"\n",
    "PS1.parent.mkdir(parents=True, exist_ok=True)\n",
    "PS1.write_text(ps, encoding=\"utf-8\")\n",
    "print(\"[OK] wrote\", PS1)\n",
    "print(\"Run in PowerShell from project root:\\n  powershell -ExecutionPolicy Bypass -File\", PS1.relative_to(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280847aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: write an Ollama Modelfile that layers the LoRA adapter\n",
    "MODELDIR = ROOT / \"configs\" / \"Modelfiles\"\n",
    "MODELDIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELFILE = MODELDIR / \"title17-lora.Modelfile\"\n",
    "\n",
    "# Point FROM to an Ollama base you already have locally.\n",
    "# If you serve llama3.1:8b-instruct-q8_0 in Ollama, keep that:\n",
    "base_ollama = \"llama3.1:8b-instruct-q8_0\"\n",
    "\n",
    "txt = f\"\"\"# Modelfile layering a llama.cpp LoRA (GGUF)\n",
    "FROM {base_ollama}\n",
    "ADAPTER {ADAPTER_OUT.as_posix()}\n",
    "\n",
    "PARAMETER temperature 0.2\n",
    "\"\"\"\n",
    "MODELFILE.write_text(txt, encoding=\"utf-8\")\n",
    "print(\"[OK] wrote\", MODELFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee27803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: show shell commands to register & run in Ollama\n",
    "print(\"Register model:\")\n",
    "print(f\"  ollama create title17-lora -f {MODELFILE.as_posix()}\")\n",
    "\n",
    "print(\"\\nTest it:\")\n",
    "print(\"  ollama run title17-lora \\\"Summarize §107 fair use in 3 bullets. Cite pages if known.\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-agent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
