{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176d019b",
   "metadata": {},
   "source": [
    "### **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d56493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent | Python: 3.11.13\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, os, platform\n",
    "\n",
    "CWD  = Path.cwd().resolve()\n",
    "ROOT = CWD if (CWD / \"src\").exists() else CWD.parent\n",
    "if str(ROOT) not in sys.path: sys.path.append(str(ROOT))\n",
    "\n",
    "print(\"ROOT:\", ROOT, \"| Python:\", platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5419e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\n",
      "CHUNKS_DIR: True\n",
      "GRAPH_DIR: True\n",
      "RERANK_DIR: True\n",
      "TITLE17_MERGED: True\n",
      "Python: 3.11.13\n"
     ]
    }
   ],
   "source": [
    "CHUNKS_DIR = ROOT / \"data\" / \"chunks\"\n",
    "GRAPH_DIR  = ROOT / \"outputs\" / \"graph\" / \"graph\"\n",
    "RERANK_DIR = ROOT / \"outputs\" / \"reranker\" / \"title17\"\n",
    "TITLE17_MERGED = ROOT / \"outputs\" / \"lora_hf\" / \"title17_merged\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"CHUNKS_DIR:\", CHUNKS_DIR.exists())\n",
    "print(\"GRAPH_DIR:\", GRAPH_DIR.exists())\n",
    "print(\"RERANK_DIR:\", RERANK_DIR.exists())\n",
    "print(\"TITLE17_MERGED:\", TITLE17_MERGED.exists())\n",
    "print(\"Python:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3dc78",
   "metadata": {},
   "source": [
    "### **Checking prompts loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710061c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] system_router: 538 chars\n",
      "[OK] system_title17: 549 chars\n",
      "[OK] output_enhancer: 746 chars\n",
      "[OK] style_rules: 667 chars\n",
      "[OK] answer_with_citations: 409 chars\n",
      "[OK] summarize_tree: 670 chars\n"
     ]
    }
   ],
   "source": [
    "from src.serve.prompt_loader import load_prompt\n",
    "\n",
    "for name in [\"system_router\",\"system_title17\",\"output_enhancer\",\"style_rules\",\"answer_with_citations\",\"summarize_tree\"]:\n",
    "    p = load_prompt(ROOT, name)\n",
    "    print(f\"[OK] {name}: {len(p)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873a6bb",
   "metadata": {},
   "source": [
    "### **Verifying retriever alone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80bb99b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pdf-agent-2\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits: 6\n",
      "#1 score=0.507 node=SEC-00017 chunk=title17-h-342\n",
      "#2 score=-0.890 node=SEC-00019 chunk=title17-h-689\n",
      "#3 score=-1.469 node=SEC-00016 chunk=title17-h-328\n",
      "\n",
      "CTX[0]:\n",
      " (untitled)\n",
      "§ 114 · Scope of exclusive rights in sound recordings 48\n",
      "[chunk title17-h-342]\n"
     ]
    }
   ],
   "source": [
    "from src.serve.retriever import GraphRetriever, prepare_contexts\n",
    "from pathlib import Path\n",
    "\n",
    "retr = GraphRetriever(ROOT/\"data/chunks\", ROOT/\"outputs/graph/graph\", ROOT/\"outputs/reranker/title17\")\n",
    "\n",
    "q = \"Summarize § 114 performance rights caveat. End with [pp. 67–88].\"\n",
    "hits = retr.search(q, k_nodes=50, k_final_nodes=8, k_each_node=14, k_final_chunks=6)\n",
    "print(\"hits:\", len(hits))\n",
    "for i,h in enumerate(hits[:3],1):\n",
    "    print(f\"#{i} score={h['score']:.3f} node={h['node_id']} chunk={h['chunk_id']}\")\n",
    "ctxs = prepare_contexts(hits, max_chars=500)\n",
    "print(\"\\nCTX[0]:\\n\", ctxs[0][:400] if ctxs else \"NO CTX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc141f",
   "metadata": {},
   "source": [
    "### **Title-17 HF wrapper directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7c329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\src\\agent\\llms.py:13: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  intro_llm  = ChatOllama(model=OLLAMA_MODEL, temperature=0.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5c4e9366e7406a921edf4ac3453303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "D:\\IIT BBS\\Job Resources\\Business Optima\\pdf-agent\\src\\agent\\llms.py:49: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  return HuggingFacePipeline(pipeline=gen_pipe)\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>  The exclusive rights granted under section 106 are subject to a royalty payment requirement for certain performances, as specified in paragraph (1).\n"
     ]
    }
   ],
   "source": [
    "from src.agent.llms import load_title17_hf\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "llm = load_title17_hf(TITLE17_MERGED)\n",
    "SYS = \"You answer strictly using the provided CONTEXTS. Cite pages if present. If not in contexts, say you don't know.\"\n",
    "question = \"Summarize § 114 performance rights caveat. End with [pp. 67–88].\"\n",
    "ctx_text = \"\\n\\n\".join(ctxs[:3]) if ctxs else \"NO CONTEXTS\"\n",
    "user = f\"CONTEXTS:\\n{ctx_text}\\n\\nUSER QUESTION:\\n{question}\\n\\nAnswer:\"\n",
    "msg = llm.invoke([SystemMessage(content=SYS), HumanMessage(content=user)])\n",
    "print(type(msg), getattr(msg, \"content\", msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d6461",
   "metadata": {},
   "source": [
    "### **Verifying Ollama models reachability (router/polisher)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786a5efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying ChatOllama (mistral:instruct)...\n",
      " Router Alive\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "print(\"Trying ChatOllama (mistral:instruct)...\")\n",
    "try:\n",
    "    test_llm = ChatOllama(model=\"mistral:instruct\", temperature=0.1)\n",
    "    print(test_llm.invoke(\"Say: 'router alive'\").content)\n",
    "except Exception as e:\n",
    "    print(\"Ollama error:\", e, \"\\nEnsure `ollama serve` is running and the model is pulled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e81e4",
   "metadata": {},
   "source": [
    "### **Router only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2847d7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b938c125bb48a481c7b13ff610bec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d7d568271e48398d4a8e28c7812162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.serve.orchestrator import run_chat\n",
    "print(\"Router GENERAL ->\", run_chat(\"hey there!\", \"sess1\")[:120], \"...\")\n",
    "print(\"Router TITLE17 ->\", run_chat(\"Summarize §114 performance right caveat.\", \"sess1\")[:160], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1bc941",
   "metadata": {},
   "source": [
    "### **Full Title-17 pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ca7960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Clarity and flow improved:\n",
      "   - Performing a work publicly under Section 114 comes with specific conditions and exceptions.\n",
      "   - Public performances via phonorecords require royalty payments to copyright owners, whereas private home viewing use is exempt.\n",
      "- Typos fixed: none identified in the provided text.\n",
      "- Conciseness maintained: the original text was concise.\n",
      "- Bullet points preserved: as requested.\n",
      "- No new claims or citations added: the draft already contains all necessary information.\n",
      "- Lightly cleaned version returned: no significant changes made beyond improving clarity and flow.\n"
     ]
    }
   ],
   "source": [
    "q = \"Summarize § 114 performance rights caveat. End with [pp. 67–88].\"\n",
    "print(run_chat(q, \"sess2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1701b1",
   "metadata": {},
   "source": [
    "### **Inspect last messages from sqlite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03161ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite not enabled or schema differs: get_last_messages() got an unexpected keyword argument 'limit'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from src.serve.sqlite_store import get_last_messages\n",
    "    msgs = get_last_messages(\"sess2\", limit=6)\n",
    "    for m in msgs: print(m)\n",
    "except Exception as e:\n",
    "    print(\"sqlite not enabled or schema differs:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10b75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-agent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
